# LLM安全防护系统工作流程详解

## 📋 目录
1. [系统启动流程](#系统启动流程)
2. [请求处理流程](#请求处理流程)
3. [敏感词检测流程](#敏感词检测流程)
4. [API调用流程](#api调用流程)
5. [错误处理流程](#错误处理流程)
6. [日志记录流程](#日志记录流程)

## 🚀 系统启动流程

### 启动时序图
```
用户 -> main.py -> LLMGuardSystem -> 配置管理器 -> 敏感词检测器 -> API客户端 -> 就绪状态
```

### 详细启动步骤

#### 1. 程序入口 (main.py)
```python
def main():
    # 解析命令行参数
    parser = argparse.ArgumentParser()
    args = parser.parse_args()
    
    # 初始化系统
    system = LLMGuardSystem(args.config)
```

#### 2. 系统初始化 (LLMGuardSystem.__init__)
```python
def __init__(self, config_file: str = "config.json"):
    # 步骤1: 加载配置文件
    self.config = self._load_config(config_file)
    
    # 步骤2: 初始化敏感词检测器
    detector_config = self.config.get("sensitive_detector", {})
    self.sensitive_detector = create_detector(detector_config)
    
    # 步骤3: 初始化API客户端
    api_config = self.config.get("api_client", {})
    self.api_client = create_api_client(api_config)
    
    # 步骤4: 初始化安全客户端
    self.safe_client = SafeLLMClient(self.api_client, self.sensitive_detector)
```

#### 3. 配置加载过程
```python
def _load_config(self, config_file: str) -> Dict[str, Any]:
    # 默认配置
    default_config = {...}
    
    # 如果配置文件存在，合并用户配置
    if os.path.exists(config_file):
        with open(config_file, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
        # 递归合并配置
        merge_config(default_config, user_config)
    
    return default_config
```

#### 4. 敏感词检测器初始化
```python
def __init__(self, sensitive_words_file="sensitive_words.txt", ...):
    # 加载敏感词列表
    self.sensitive_words = self._load_sensitive_words()
    
    # 初始化llm-guard的BanSubstrings扫描器
    self.scanner = BanSubstrings(
        substrings=self.sensitive_words,
        match_type=self.match_type,
        case_sensitive=self.case_sensitive,
        redact=self.redact,
        contains_all=self.contains_all
    )
```

#### 5. API客户端初始化
```python
def __init__(self, api_key, base_url, model, ...):
    # 初始化OpenAI客户端（兼容通义千问）
    self.client = OpenAI(
        api_key=self.api_key,
        base_url=self.base_url
    )
```

## 🔄 请求处理流程

### 主要处理函数
```python
def process_request(self, user_input: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
```

### 处理流程图
```
用户输入 
    ↓
[1] 开始计时
    ↓
[2] 获取安全配置
    ↓
[3] 调用安全客户端
    ↓
[4] 输入安全检查
    ↓
[5] API调用 (如果输入安全)
    ↓
[6] 输出安全检查
    ↓
[7] 生成结果对象
    ↓
[8] 记录日志
    ↓
[9] 返回结果
```

### 详细步骤实现

#### 步骤1-2: 初始化和配置
```python
def process_request(self, user_input: str, system_prompt: Optional[str] = None):
    start_time = datetime.now()  # 开始计时
    
    # 获取安全配置
    safety_config = self.config.get("safety", {})
    check_input = safety_config.get("check_input", True)
    check_output = safety_config.get("check_output", True)
    log_blocked = safety_config.get("log_blocked_requests", True)
```

#### 步骤3: 调用安全客户端
```python
    # 调用安全客户端进行处理
    result = self.safe_client.safe_chat(
        user_message=user_input,
        system_message=system_prompt,
        check_input=check_input,
        check_output=check_output
    )
```

#### 步骤4-9: 后处理和返回
```python
    # 添加处理时间和时间戳
    end_time = datetime.now()
    result["processing_time"] = (end_time - start_time).total_seconds()
    result["timestamp"] = end_time.isoformat()
    
    # 记录被阻止的请求
    if result.get("blocked") and log_blocked:
        self._log_blocked_request(user_input, result)
    
    return result
```

## 🔍 敏感词检测流程

### 检测函数
```python
def scan(self, text: str) -> Tuple[str, bool, float]:
```

### 检测流程图
```
输入文本
    ↓
[1] 文本预处理
    ↓
[2] BanSubstrings扫描
    ↓
[3] 匹配结果分析
    ↓
[4] 风险评分计算
    ↓
[5] 内容替换 (如果需要)
    ↓
[6] 返回结果
```

### 详细实现

#### BanSubstrings扫描过程
```python
def scan(self, text: str) -> Tuple[str, bool, float]:
    if not text:
        return text, True, 0.0
    
    try:
        # 调用llm-guard的扫描器
        sanitized_text, is_valid, risk_score = self.scanner.scan(text)
        
        if not is_valid:
            logger.warning(f"检测到敏感词，风险评分: {risk_score}")
            logger.debug(f"原文本: {text[:100]}...")
            logger.debug(f"处理后: {sanitized_text[:100]}...")
        
        return sanitized_text, is_valid, risk_score
        
    except Exception as e:
        logger.error(f"敏感词扫描失败: {e}")
        return text, False, 1.0
```

#### 风险评分机制
```python
# llm-guard内部的评分逻辑
if 检测到敏感词:
    risk_score = 1.0    # 高风险
    is_valid = False    # 不安全
else:
    risk_score = -1.0   # 安全
    is_valid = True     # 安全
```

#### 内容替换机制
```python
# 当redact=True时，敏感词会被替换
原文: "这个内容包含敏感词汇"
处理后: "这个内容包含[REDACT]"
```

## 🌐 API调用流程

### 安全客户端处理
```python
def safe_chat(self, user_message: str, system_message: Optional[str] = None,
              check_input: bool = True, check_output: bool = True) -> Dict[str, Any]:
```

### API调用流程图
```
用户消息
    ↓
[1] 初始化结果对象
    ↓
[2] 输入安全检查 (可选)
    ↓
[3] 检查是否被阻止
    ↓
[4] 调用LLM API
    ↓
[5] 输出安全检查 (可选)
    ↓
[6] 最终结果处理
    ↓
返回结果对象
```

### 详细实现

#### 步骤1: 初始化结果对象
```python
result = {
    "original_input": user_message,
    "input_safe": True,
    "input_risk_score": 0.0,
    "sanitized_input": user_message,
    "response": "",
    "output_safe": True,
    "output_risk_score": 0.0,
    "sanitized_output": "",
    "blocked": False,
    "block_reason": ""
}
```

#### 步骤2: 输入安全检查
```python
if check_input and self.sensitive_detector:
    sanitized_input, input_safe, input_risk = self.sensitive_detector.scan(user_message)
    result.update({
        "input_safe": input_safe,
        "input_risk_score": input_risk,
        "sanitized_input": sanitized_input
    })
    
    if not input_safe:
        result.update({
            "blocked": True,
            "block_reason": "输入包含敏感词",
            "response": "抱歉，您的输入包含不当内容，无法处理。"
        })
        return result
```

#### 步骤4: LLM API调用
```python
try:
    response = self.api_client.simple_chat(user_message, system_message)
    result["response"] = response
except Exception as e:
    result.update({
        "blocked": True,
        "block_reason": f"API调用失败: {str(e)}",
        "response": "抱歉，服务暂时不可用，请稍后重试。"
    })
```

#### 步骤5: 输出安全检查
```python
if check_output and self.sensitive_detector:
    sanitized_output, output_safe, output_risk = self.sensitive_detector.scan(response)
    result.update({
        "output_safe": output_safe,
        "output_risk_score": output_risk,
        "sanitized_output": sanitized_output
    })
    
    if not output_safe:
        result.update({
            "blocked": True,
            "block_reason": "输出包含敏感词",
            "response": "抱歉，模型生成的内容包含不当信息，已被过滤。",
            "sanitized_output": sanitized_output
        })
```

## ❌ 错误处理流程

### 错误类型和处理策略

#### 1. 配置文件错误
```python
try:
    with open(config_file, 'r', encoding='utf-8') as f:
        user_config = json.load(f)
except FileNotFoundError:
    logger.warning("配置文件不存在，使用默认配置")
    user_config = {}
except json.JSONDecodeError as e:
    logger.error(f"配置文件格式错误: {e}")
    user_config = {}
```

#### 2. 敏感词文件错误
```python
try:
    with open(self.sensitive_words_file, 'r', encoding='utf-8') as f:
        words = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    logger.error(f"敏感词文件不存在: {self.sensitive_words_file}")
    return []
except UnicodeDecodeError as e:
    logger.error(f"敏感词文件编码错误: {e}")
    return []
```

#### 3. API调用错误
```python
try:
    response = self.client.chat.completions.create(**params)
except openai.APIError as e:
    logger.error(f"API错误: {e}")
    raise
except openai.RateLimitError as e:
    logger.warning(f"API限流: {e}")
    time.sleep(1)  # 等待后重试
    raise
except Exception as e:
    logger.error(f"未知错误: {e}")
    raise
```

#### 4. 敏感词检测错误
```python
try:
    sanitized_text, is_valid, risk_score = self.scanner.scan(text)
    return sanitized_text, is_valid, risk_score
except Exception as e:
    logger.error(f"敏感词扫描失败: {e}")
    # 安全优先：检测失败时认为不安全
    return text, False, 1.0
```

## 📝 日志记录流程

### 日志配置
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_guard.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
```

### 日志记录点

#### 1. 系统启动日志
```python
logger.info("LLM安全防护系统初始化完成")
logger.info(f"加载了 {len(self.sensitive_words)} 个敏感词")
logger.info(f"使用模型: {self.model}")
```

#### 2. 请求处理日志
```python
logger.info(f"处理用户请求: {user_input[:50]}...")
logger.info(f"API调用成功，使用token: {total_tokens}")
```

#### 3. 安全事件日志
```python
logger.warning(f"检测到敏感词，风险评分: {risk_score}")
logger.warning(f"请求被阻止: {block_reason}")
```

#### 4. 错误日志
```python
logger.error(f"API调用失败: {e}")
logger.error(f"敏感词扫描失败: {e}")
```

### 被阻止请求的详细日志
```python
def _log_blocked_request(self, user_input: str, result: Dict[str, Any]):
    log_entry = {
        "timestamp": result["timestamp"],
        "user_input": user_input[:100] + "..." if len(user_input) > 100 else user_input,
        "block_reason": result["block_reason"],
        "input_risk_score": result.get("input_risk_score", 0),
        "output_risk_score": result.get("output_risk_score", 0)
    }
    
    logger.warning(f"请求被阻止: {json.dumps(log_entry, ensure_ascii=False)}")
```

---

通过这个详细的工作流程文档，您可以清楚地了解系统的每个环节是如何工作的，以及各个组件之间是如何协调配合的。这个系统采用了多层防护、错误安全优先的设计理念，确保在任何情况下都能提供可靠的安全防护。
